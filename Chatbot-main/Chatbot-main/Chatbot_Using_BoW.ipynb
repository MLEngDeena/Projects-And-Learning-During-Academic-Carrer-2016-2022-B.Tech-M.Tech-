{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chatbot Using BoW.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNKT5aZn1/pZ8fvCl/QTSr+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MLEngDeena/Chatbot/blob/main/Chatbot_Using_BoW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing The Libraries"
      ],
      "metadata": {
        "id": "9ltYObM26Zr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as  np\n",
        "import nltk\n",
        "import string\n",
        "import random"
      ],
      "metadata": {
        "id": "GD3emzuG6bMo"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing and reading the corpus"
      ],
      "metadata": {
        "id": "ZKAvIcio6oUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('chatbot.txt' , 'r', errors = 'ignore')\n",
        "raw_doc = f.read()\n",
        "raw_doc = raw_doc.lower() # converts text to lowercase\n",
        "nltk.download('punkt') # Using the Punkt tokenizer\n",
        "nltk.download('wordnet') # Using the WordNet dictionary\n",
        "sent_tokens = nltk.sent_tokenize(raw_doc) #Converts doc to list of sentences\n",
        "word_tokens = nltk.word_tokenize(raw_doc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lFaDKhiQ6lfO",
        "outputId": "7ba2977b-4a1f-4d7f-cdd1-9fe439a0304b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example of Sentence and word token"
      ],
      "metadata": {
        "id": "OiSuqCQX-fBh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(sent_tokens[:2])\n",
        "print(word_tokens[:2])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGnDaA9Q_FKQ",
        "outputId": "d0758f25-d720-408c-d9f3-c0fee868b02a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['data science\\nfrom wikipedia, the free encyclopedia\\njump to navigationjump to search\\nnot to be confused with information science.', 'the existence of comet neowise (here depicted as a series of red dots) was discovered by analyzing astronomical survey data acquired by a space telescope, the wide-field infrared survey explorer.']\n",
            "['data', 'science']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Pre-Processing"
      ],
      "metadata": {
        "id": "SISI_EIx_eM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmer = nltk.stem.WordNetLemmatizer()\n",
        "#WordNet is a semantically-orinted dictionary of English included in NLTK.\n",
        "\n",
        "def LemTokens(tokens):\n",
        "  return [lemmer.lemmatize(token) for token in tokens]\n",
        "\n",
        "remove_punct_dict = dict((ord(punct) , None) for punct in string.punctuation)\n",
        "\n",
        "def LemNormalize(text):\n",
        "  return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punct_dict)))"
      ],
      "metadata": {
        "id": "N098kHo0_nR2"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining the greeting function"
      ],
      "metadata": {
        "id": "57T67iYGA3Gd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GREET_INPUTS = (\"hello\" , \"hi\" , \"greetings\" , \"sup\" , \"what's up\" , \"hey\")\n",
        "GREET_RESPONSES = [\"hi\" , \"hey\" , \"*nods*\" , \"hi there\" , \"hello\" , \"I am glad! You are taking to me\"]\n",
        "\n",
        "def greet(sentence):\n",
        "  for word in sentence.split():\n",
        "    if word.lower() in GREET_INPUTS:\n",
        "      return random.choice(GREET_RESPONSES)"
      ],
      "metadata": {
        "id": "WMvghEnPA8n-"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Response Generation"
      ],
      "metadata": {
        "id": "9VQRBd0FBrul"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity"
      ],
      "metadata": {
        "id": "bkLP_aF0Btts"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def response(user_respone):\n",
        "  robo1_response = ''\n",
        "  TfidfVec = TfidfVectorizer(tokenizer = LemNormalize , stop_words = 'english')\n",
        "  tfidf = TfidfVec.fit_transform(sent_tokens)\n",
        "  vals = cosine_similarity(tfidf[-1] , tfidf)\n",
        "  idx = vals.argsort()[0][-2]\n",
        "  flat = vals.flatten()\n",
        "  flat = vals.flatten()\n",
        "  flat.sort()\n",
        "  req_tfidf = flat[-2]\n",
        "  if(req_tfidf == 0 ):\n",
        "    robo1_response = robo1_response + \"I am sorry! I don't understand you\"\n",
        "    return robo1_response\n",
        "  else:\n",
        "    robo1_response = robo1_response + sent_tokens[idx]\n",
        "    return robo1_response"
      ],
      "metadata": {
        "id": "5uY9suY8Dc29"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Defining conversation start/end protocols"
      ],
      "metadata": {
        "id": "15oVO4SLEp59"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flag = True\n",
        "print(\"BOT : My name is Stark. Let's Have a conversation! Also, if you want to exit any time , just type Bye! \")\n",
        "while(flag==True):\n",
        "  user_response = input()\n",
        "  user_response = user_response.lower()\n",
        "  if(user_response != 'bye'):\n",
        "    if(user_response == 'thanks' or user_response == 'thank you'):\n",
        "      flag = False\n",
        "      print(\"BOT : you are Welcome..\")\n",
        "    else:\n",
        "      if(greet(user_response) != None):\n",
        "        print(\"BOT: \"+greet(user_response))\n",
        "      else:\n",
        "        sent_tokens.append(user_response)\n",
        "        word_tokens = word_tokens + nltk.word_tokenize(user_response)\n",
        "        final_words = list(set(word_tokens))\n",
        "        print(\"BOT : \" , end = \"\")\n",
        "        print(response(user_response))\n",
        "        sent_tokens.remove(user_response)\n",
        "  else:\n",
        "    flag = False\n",
        "    print(\"BOT: Goodbye! Take Care < 3 \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Oi9IYCQAEWNh",
        "outputId": "3fc26bf7-61a9-429d-ddd6-8f47666c3fe9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BOT : My name is Stark. Let's Have a conversation! Also, if you want to exit any time , just type Bye! \n",
            "hello\n",
            "BOT: hello\n",
            "hi\n",
            "BOT: hi there\n",
            "foundation\n",
            "BOT : "
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[7]\n",
            "\n",
            "\n",
            "contents\n",
            "1\tfoundations\n",
            "1.1\trelationship to statistics\n",
            "2\tetymology\n",
            "2.1\tearly usage\n",
            "2.2\tmodern usage\n",
            "3\ttechnologies and techniques\n",
            "4\tsee also\n",
            "5\treferences\n",
            "foundations\n",
            "data science is an interdisciplinary field focused on extracting knowledge from data sets, which are typically large (see big data), and applying the knowledge and actionable insights from data to solve problems in a wide range of application domains.\n",
            "data science\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "BOT : \"about data science\".\n",
            "impact of data science\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/feature_extraction/text.py:401: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
            "  % sorted(inconsistent)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BOT : \"how data science will impact future of businesses?\".\n",
            "bye\n",
            "BOT: Goodbye! Take Care < 3 \n"
          ]
        }
      ]
    }
  ]
}